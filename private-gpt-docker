#!/bin/bash
clear

# check if we are using a desktop with nvidia drivers installed
check_nvidia_smi() {
  if command -v nvidia-smi > /dev/null 2>&1; then
    return 0  # nvidia-smi is available
  else
    return 1  # nvidia-smi is not available
  fi
}

# check if we are using a Jetson
check_jetson_lshw() {
  if sudo lshw | grep -q "Jetson"; then
    return 0  # Jetson identified
  else
    return 1  # Jetson not identified
  fi
}

# Main check
if check_nvidia_smi; then
  echo "NVIDIA drivers detected"
elif check_jetson_lshw; then
  echo "NVIDIA Jetson identified"
else
  echo "NVIDIA GPU not detected, aborting"
  exit 1
fi
echo "\n\n"
mkdir -p service_units

# get vars
deploy_ux=1
default_port=8080
port="${1:-$default_port}"
default_instance=private-gpt-ollama-client
instance="${2:-$default_instance}"

echo "Deploy default UX? [Y/N]"
echo Hint: If you are connecting a custom UX from the host, select no.
read -r answer


# Process the user's response
if [[ "$answer" =~ ^[Nn]$ ]]; then
    deploy_ux=0
fi

# Check if Docker is installed
if ! command -v docker > /dev/null 2>&1; then
    echo "Docker is not installed. Installing Docker..."
    sudo apt-get update
    sudo apt-get install -y docker.io
fi

# check is ollama is instaled on the host
service_file="/etc/systemd/system/ollama.service"
if [ ! -f "$service_file" ]; then
    echo "ollama not found, installing"
    curl -fsSL https://ollama.com/install.sh | sh
    ollama pull aya:8b
    ollama serve
else
    echo "ollama install detected, proceding"
fi

# check that we are listening on all adaptors
env_var='Environment="OLLAMA_HOST=0.0.0.0"'

# Check if the Environment variable line is missing
temp_file=$(mktemp)
if ! grep -q '^Environment="OLLAMA_HOST=' "$service_file"; then
    awk -v env_var="$env_var" '
    BEGIN {in_service_section=0}
    /^\[Service\]$/ {in_service_section=1}
    /^\[Install\]$/ {in_service_section=0}
    {
        print
        if (in_service_section && !/^Environment="OLLAMA_HOST=/) {
            if (/^ExecStart/) {
                print env_var
            }
        }
    }
    ' "$service_file" > "$temp_file"

    # Replace the original file with the updated one
    sudo mv "$temp_file" "$service_file"
    echo "Added $env_var to the [Service] section of $service_file - restarting service"
    sudo systemctl daemon-reload
    sudo service ollama restart
else
    echo "$env_var already exists in $service_file"
fi

git clone https://github.com/zylon-ai/private-gpt.git

# update listening port
file="Dockerfile.ollama-client"
echo "setting listening port to $port"
sed -i "s|ENV PORT=[0-9]*|ENV PORT=$port|" "$file"
sed -i "s|EXPOSE [0-9]*|EXPOSE $port|" "$file"
cp ./Dockerfile.ollama-client ./private-gpt/

######## TEMPORARY TOKEN_LIMIT FIX #######

cp chat_service.py private-gpt/private_gpt/server/chat/

cd private-gpt || exit

echo "Enable PyTorch (GPU will be used if available)? [Y/N]"
read -r answer
# Process the user's response
if [[ "$answer" =~ ^[Yy]$ ]]; then
    echo enabling torch
    sed -i '/torch =.*optional = true/ s/optional = true/optional = false/' pyproject.toml
else
    echo disabling torch
    sed -i '/torch =.*optional = false/ s/optional = false/optional = true/' pyproject.toml
fi

# update the ollama address to the docker host
# https://ollama.com/library/aya/blobs/495401f864a6
file="./settings-ollama.yaml"
sed -i 's|context_window: 3900|context_window: 8192|' "$file"
# sed -i 's|max_new_tokens: 512|max_new_tokens: 14336|' "$file"
sed -i 's|llm_model: llama3.1|llm_model: aya:8b|' "$file"
sed -i 's|api_base: http://localhost:11434|api_base: http://172.17.0.1:11434|' "$file"
sed -i 's|embedding_api_base: http://localhost:11434|embedding_api_base: http://172.17.0.1:11434|' "$file"

# update the embedding settings
new_embedding_content="embedding:
  mode: ollama
  embed_dim: 768
  ingest_mode: pipeline
  count_workers: 4

"

temp_file=$(mktemp)

# Use awk to update the embedding section
awk -v new_content="$new_embedding_content" '
BEGIN {section=0}
{
    if ($0 ~ /^embedding:/) {
        section=1
        print new_content
        next
    }
    if (section && $0 ~ /^[a-zA-Z]/ && $0 !~ /^[[:space:]]/) {
        section=0
    }
    if (section) {
        next
    }
    print
}
' "$file" > "$temp_file"

# Replace the original file with the updated temporary file
sudo mv "$temp_file" "$file"

file="settings.yaml"
# enable or disable the UX
if [ "$deploy_ux" -eq 0 ]; then
    echo setting UX to disabled
    sed -i '/^\s*ui:/,/^\s*[^ ]/s/^\(\s*enabled:\) true/\1 false/' "$file"
else
    echo setting UX to enabled
    sed -i '/^\s*ui:/,/^\s*[^ ]/s/^\(\s*enabled:\) false/\1 true/' "$file"
fi

# start docker build
sudo docker build -f Dockerfile.ollama-client -t "$instance":latest .

# create service unit
filename="$instance.service"
exec_start_statement="/usr/bin/docker start -a $instance"
exec_stop_statement="/usr/bin/docker stop -t 10 $instance"

unit_contents="[Unit]
Description=\"$instance\"
After=network-online.target network.target rsyslog.service ollama.service docker.socket
Requires=docker.socket

[Service]
Type=simple
ExecStart=$exec_start_statement
ExecStop=$exec_stop_statement
TimeoutSec=60
User=root

[Install]
WantedBy=multi-user.target"

echo "$unit_contents" > "$filename"
echo "Unit file '$filename' has been created."
mv "$filename" ../service_units
cd ../service_units || return

echo build complete creating instance
if [ "$deploy_ux" -eq 0 ]; then
    echo build specified as API only, container will listen on localhost:"port"
    sudo docker run -d -p 127.0.0.1:"$port":"$port" --name "$instance" "$instance":latest /bin/bash
else
    echo default UX has been deployed, container will listen on 0.0.0.0:"$port"
    sudo docker run -d -p "$port:$port" --name "$instance" "$instance":latest /bin/bash
fi
sleep 20
sudo docker stop -t 10 $instance

echo "creation of \"$instance\" complete, create service? [Y/N]"
read -r answer

# Process the user's response
if [[ "$answer" =~ ^[Yy]$ ]]; then
    # If the answer is yes, create the symbolic link
    sudo ln -s "$(pwd)/$filename" /etc/systemd/system/"$filename"
    sudo systemctl daemon-reload
    sudo systemctl enable "$filename"
    echo "Service created and linked to /etc/systemd/system/, starting service..."
    sudo systemctl start "$filename"
    sudo journalctl -u "$filename" -f
fi
